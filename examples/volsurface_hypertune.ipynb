{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4d1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.insert(0, os.getenv('SRC_PATH'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from src.volsurface import GridInterpVolSurface, KernelVolSurface\n",
    "from src.utils.data_helper import clean_data, VolSurfPointwiseDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from src.train import Trainer\n",
    "\n",
    "from src.volsurface import VAEPWVolSurface\n",
    "# from src.volsurface import TrainedDecoderVolSurface\n",
    "\n",
    "import json\n",
    "\n",
    "DB_PATH = os.getenv('DB_PATH')\n",
    "CSV_PATH = os.getenv('CSV_PATH')\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383566dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.logger import setup_logger\n",
    "logger = setup_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b33b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-20 14:13:40] [INFO] src.utils.data_helper (50) : Bad data - Filtered 1110238 rows, Retained sample 76.67%\n",
      "[2025-04-20 14:13:51] [INFO] src.utils.data_helper (63) : Consecutive trading stats completed\n",
      "[2025-04-20 14:13:52] [INFO] src.utils.data_helper (86) : Consecutive trading - Filtered 2341950 rows, Retained sample 35.81%\n",
      "[2025-04-20 14:13:52] [INFO] src.utils.data_helper (91) : Moneyness calculation completed\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT date, symbol, exdate, last_date, cp_flag, strike_price, best_bid, best_offer, volume, open_interest, impl_volatility, delta\n",
    "FROM opprc\n",
    "\"\"\"\n",
    "dtype = {\n",
    "    'symbol': 'string',\n",
    "    'cp_flag': 'string',\n",
    "    'strike_price': 'float64',\n",
    "    'best_bid': 'float64',\n",
    "    'best_offer': 'float64',\n",
    "    'volume': 'int64',\n",
    "    'open_interest': 'int64',\n",
    "    'impl_volatility': 'float64',\n",
    "    'delta': 'float64'\n",
    "}\n",
    "df_raw = pd.read_sql_query(query, conn, parse_dates=['date', 'exdate', 'last_date'])\n",
    "df_raw = df_raw.replace('', np.nan) # sqlite returns empty strings for NULL values\n",
    "df_raw = df_raw.astype(dtype)\n",
    "\n",
    "df = clean_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2fedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vae_pw_ii_tune\"\n",
    "train_model = True\n",
    "load_model = False\n",
    "save_model = False\n",
    "data_dir = CSV_PATH + \"/predicted_vol_surfaces.json\"  # Path to the volatility surfaces dataset\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acf7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "maturity_grid = np.array([1, 7, 30, 60, 90, 180, 360, 720])\n",
    "delta_grid = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148c39a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-20 14:14:05] [INFO] src.train (60) : Using device: mps\n"
     ]
    }
   ],
   "source": [
    "SRC_PATH = os.getenv('SRC_PATH')\n",
    "os.chdir(SRC_PATH)\n",
    "trainer = Trainer(model_name)\n",
    "trainer.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b6aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "mapping_ids = {dt: i for i, dt in enumerate(sorted(map(lambda x: x[:10], data.keys())))}\n",
    "\n",
    "df['mapping_ids'] = df['date'].dt.strftime('%Y-%m-%d').map(mapping_ids)\n",
    "\n",
    "vol_surfaces = []\n",
    "for key in sorted(data.keys()):\n",
    "    surface = torch.tensor(data[key], dtype=torch.float32)\n",
    "    vol_surfaces.append(surface.flatten())  # Flatten 2D to 1D\n",
    "\n",
    "data_tensor = torch.stack(vol_surfaces)\n",
    "pw_grid_data = torch.tensor(df[['ttm', 'moneyness']].values, dtype=torch.float32)\n",
    "# !only for test run\n",
    "pw_grid_data[:, 0] = pw_grid_data[:, 0] / 365.0\n",
    "pw_vol_data = torch.tensor(df['impl_volatility'].values, dtype=torch.float32)\n",
    "mapping_ids = torch.tensor(df['mapping_ids'].values).long()\n",
    "\n",
    "assert data_tensor.shape[0] == max(mapping_ids) + 1\n",
    "\n",
    "dataset = VolSurfPointwiseDataset(pw_grid_data, pw_vol_data, data_tensor, mapping_ids)\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=trainer.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe838e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-20 14:14:14] [INFO] train (3) : Epoch 1/10\n",
      "[2025-04-20 14:15:25] [INFO] src.train (179) : Loss: 0.0050\n",
      "[2025-04-20 14:15:25] [INFO] train (3) : Epoch 2/10\n",
      "[2025-04-20 14:16:35] [INFO] src.train (179) : Loss: 0.0041\n",
      "[2025-04-20 14:16:35] [INFO] train (3) : Epoch 3/10\n",
      "[2025-04-20 14:17:47] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-20 14:17:47] [INFO] train (3) : Epoch 4/10\n",
      "[2025-04-20 14:18:56] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-20 14:18:56] [INFO] train (3) : Epoch 5/10\n",
      "[2025-04-20 14:20:03] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-20 14:20:03] [INFO] train (3) : Epoch 6/10\n",
      "[2025-04-20 14:21:10] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-20 14:21:10] [INFO] train (3) : Epoch 7/10\n",
      "[2025-04-20 14:22:17] [INFO] src.train (179) : Loss: 0.0038\n",
      "[2025-04-20 14:22:17] [INFO] train (3) : Epoch 8/10\n",
      "[2025-04-20 14:23:25] [INFO] src.train (179) : Loss: 0.0038\n",
      "[2025-04-20 14:23:25] [INFO] train (3) : Epoch 9/10\n",
      "[2025-04-20 14:24:33] [INFO] src.train (179) : Loss: 0.0038\n",
      "[2025-04-20 14:24:33] [INFO] train (3) : Epoch 10/10\n",
      "[2025-04-20 14:25:41] [INFO] src.train (179) : Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    trainer.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2363b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.model.state_dict(), f\"params/{trainer.model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cfd3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypertune!! {'learning_rate': <ray.tune.search.sample.Categorical object at 0x3a03bbf70>, 'batch_size': <ray.tune.search.sample.Categorical object at 0x3a03bb3d0>, 'latent_dim': <ray.tune.search.sample.Categorical object at 0x3a0274b80>, 'hidden_dim': <ray.tune.search.sample.Categorical object at 0x3a0274670>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 14:32:43,166\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-04-20 14:32:43,722\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2025-04-20 14:32:43,723\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745173963.905839 2771397 tcp_posix.cc:596] recvmsg encountered uncommon error: Message too long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (50 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:32:44 (running for 00:00:00.74)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 PENDING)\n",
      "+--------------------+----------+-------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc   |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | PENDING  |       |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | PENDING  |       |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=51500)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=51500)\u001b[0m E0000 00:00:1745173965.019356 2771477 tcp_posix.cc:596] recvmsg encountered uncommon error: Message too long\n",
      "\u001b[33m(raylet)\u001b[0m I0000 00:00:1745173965.492042 2770972 chttp2_transport.cc:1182] ipv4:127.0.0.1:61579: Got goaway [2] err=UNAVAILABLE:GOAWAY received; Error code: 2; Debug Text: Cancelling all calls {grpc_status:14, http2_error:2, created_time:\"2025-04-20T14:32:45.492041-04:00\", file_line:1171, file:\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-04-20 14:32:49 (running for 00:00:05.82)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:32:54 (running for 00:00:10.88)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:32:59 (running for 00:00:15.93)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:04 (running for 00:00:20.98)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:09 (running for 00:00:26.03)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:14 (running for 00:00:31.09)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:19 (running for 00:00:36.14)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:25 (running for 00:00:41.19)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:30 (running for 00:00:46.24)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:35 (running for 00:00:51.30)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:40 (running for 00:00:56.35)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:45 (running for 00:01:01.40)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:50 (running for 00:01:06.46)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:33:55 (running for 00:01:11.51)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-04-20 14:34:00 (running for 00:01:16.56)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | RUNNING  | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 14:34:03,898\tERROR tune_controller.py:1331 -- Trial task failed for trial lambda_d95d8_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=51500, ip=127.0.0.1, actor_id=f73d17d4fcf13cd27a0d26cf01000000, repr=<lambda>)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/zlx/Desktop/2025Spring/COS513/VAE-VolSurface/src/train.py\", line 220, in <lambda>\n",
      "    lambda config: self.hyper_train(train_loader, config),\n",
      "  File \"/Users/zlx/Desktop/2025Spring/COS513/VAE-VolSurface/src/train.py\", line 212, in hyper_train\n",
      "    tune.report({'loss':loss})\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "\u001b[36m(<lambda> pid=51500)\u001b[0m [2025-04-20 14:34:03] [INFO] src.train (179) : Loss: 0.0101\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(pid=51501)\u001b[0m E0000 00:00:1745173965.070168 2771528 tcp_posix.cc:596] recvmsg encountered uncommon error: Message too long\n",
      "\u001b[33m(raylet)\u001b[0m I0000 00:00:1745174043.909996 2770972 chttp2_transport.cc:1182] ipv4:127.0.0.1:61601: Got goaway [2] err=UNAVAILABLE:GOAWAY received; Error code: 2; Debug Text: Cancelling all calls {file:\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc\", file_line:1171, created_time:\"2025-04-20T14:34:03.909994-04:00\", http2_error:2, grpc_status:14}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-04-20 14:34:05 (running for 00:01:21.57)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (1 ERROR, 1 RUNNING)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00001 | RUNNING  | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "| lambda_d95d8_00000 | ERROR    | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "Number of errored trials: 1\n",
      "+--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name         |   # failures | error file                                                                                                                                                                                                                                     |\n",
      "|--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| lambda_d95d8_00000 |            1 | /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts/lambda_d95d8_00000_0_batch_size=128,hidden_dim=50,latent_dim=20,learning_rate=0.0001_2025-04-20_14-32-44/error.txt |\n",
      "+--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 14:34:06,707\tERROR tune_controller.py:1331 -- Trial task failed for trial lambda_d95d8_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/worker.py\", line 2755, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/_private/worker.py\", line 906, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=51501, ip=127.0.0.1, actor_id=c095f556d52f0deb27381f3301000000, repr=<lambda>)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/zlx/Desktop/2025Spring/COS513/VAE-VolSurface/src/train.py\", line 220, in <lambda>\n",
      "    lambda config: self.hyper_train(train_loader, config),\n",
      "  File \"/Users/zlx/Desktop/2025Spring/COS513/VAE-VolSurface/src/train.py\", line 212, in hyper_train\n",
      "    tune.report({'loss':loss})\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "2025-04-20 14:34:06,711\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/zlx/ray_results/lambda_2025-04-20_14-32-43' in 0.0021s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-04-20 14:34:06 (running for 00:01:22.86)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts\n",
      "Number of trials: 2/2 (2 ERROR)\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "| Trial name         | status   | loc             |   batch_size |   hidden_dim |   latent_dim |   learning_rate |\n",
      "|--------------------+----------+-----------------+--------------+--------------+--------------+-----------------|\n",
      "| lambda_d95d8_00000 | ERROR    | 127.0.0.1:51500 |          128 |           50 |           20 |          0.0001 |\n",
      "| lambda_d95d8_00001 | ERROR    | 127.0.0.1:51501 |           64 |          100 |           10 |          0.001  |\n",
      "+--------------------+----------+-----------------+--------------+--------------+--------------+-----------------+\n",
      "Number of errored trials: 2\n",
      "+--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name         |   # failures | error file                                                                                                                                                                                                                                     |\n",
      "|--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| lambda_d95d8_00000 |            1 | /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts/lambda_d95d8_00000_0_batch_size=128,hidden_dim=50,latent_dim=20,learning_rate=0.0001_2025-04-20_14-32-44/error.txt |\n",
      "| lambda_d95d8_00001 |            1 | /tmp/ray/session_2025-04-20_14-32-38_050488_51306/artifacts/2025-04-20_14-32-43/lambda_2025-04-20_14-32-43/driver_artifacts/lambda_d95d8_00001_1_batch_size=64,hidden_dim=100,latent_dim=10,learning_rate=0.0010_2025-04-20_14-32-44/error.txt |\n",
      "+--------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [lambda_d95d8_00000, lambda_d95d8_00001])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# hyper tune\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypertune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/2025Spring/COS513/VAE-VolSurface/src/train.py:219\u001b[0m, in \u001b[0;36mTrainer.hypertune\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHypertune!!\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_hypertune_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypertune_param))\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Run the grid search\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_hypertune_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypertune_param\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FIX: Allocate resources ---> shoule be available in yaml\u001b[39;49;00m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of samples per configuration\u001b[39;49;00m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_reporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreporter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis\u001b[38;5;241m.\u001b[39mget_best_config(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/vae-volsurface/lib/python3.10/site-packages/ray/tune/tune.py:1035\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [lambda_d95d8_00000, lambda_d95d8_00001])"
     ]
    }
   ],
   "source": [
    "# hyper tune\n",
    "trainer.hypertune(\n",
    "    train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38f9134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-19 23:47:24] [INFO] src.train (60) : Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# try\n",
    "\n",
    "model_name = \"vae_pw_ii_tune\"\n",
    "train_model = True\n",
    "load_model = False\n",
    "save_model = False\n",
    "data_dir = CSV_PATH + \"/predicted_vol_surfaces.json\"  # Path to the volatility surfaces dataset\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "trainer_tune = Trainer(model_name)\n",
    "trainer_tune.create_model()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=trainer_tune.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c65678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-19 23:47:38] [INFO] train (3) : Epoch 1/10\n",
      "[2025-04-19 23:48:49] [INFO] src.train (179) : Loss: 0.0047\n",
      "[2025-04-19 23:48:49] [INFO] train (3) : Epoch 2/10\n",
      "[2025-04-19 23:49:58] [INFO] src.train (179) : Loss: 0.0040\n",
      "[2025-04-19 23:49:58] [INFO] train (3) : Epoch 3/10\n",
      "[2025-04-19 23:51:08] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-19 23:51:08] [INFO] train (3) : Epoch 4/10\n",
      "[2025-04-19 23:52:18] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-19 23:52:18] [INFO] train (3) : Epoch 5/10\n",
      "[2025-04-19 23:53:28] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-19 23:53:28] [INFO] train (3) : Epoch 6/10\n",
      "[2025-04-19 23:54:38] [INFO] src.train (179) : Loss: 0.0039\n",
      "[2025-04-19 23:54:38] [INFO] train (3) : Epoch 7/10\n",
      "[2025-04-19 23:55:48] [INFO] src.train (179) : Loss: 0.0038\n",
      "[2025-04-19 23:55:48] [INFO] train (3) : Epoch 8/10\n",
      "[2025-04-19 23:56:58] [INFO] src.train (179) : Loss: 0.0038\n",
      "[2025-04-19 23:56:58] [INFO] train (3) : Epoch 9/10\n",
      "[2025-04-19 23:58:07] [INFO] src.train (179) : Loss: 0.0038\n",
      "[2025-04-19 23:58:07] [INFO] train (3) : Epoch 10/10\n",
      "[2025-04-19 23:59:17] [INFO] src.train (179) : Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    trainer_tune.train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a6f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae-volsurface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
